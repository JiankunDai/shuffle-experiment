package edu.ecnu

import org.apache.spark.sql.{SQLContext, DataFrame}
import org.apache.spark.SparkContext
import scala.util.Random

object DataGenerator {
  
  /**
   * ç”Ÿæˆç”¨æˆ·è¡Œä¸ºæ•°æ®
   * @param numRecords è®°å½•æ¡æ•°
   * @param numPartitions æ˜¾å¼æŒ‡å®šRDDçš„åˆ†åŒºæ•°ï¼Œè¿™å¯¹Hash Shuffleçš„æ–‡ä»¶æ•°é‡è‡³å…³é‡è¦
   */
  def generateUserBehavior(sqlContext: SQLContext, numRecords: Long, numPartitions: Int): DataFrame = {
    import sqlContext.implicits._
    
    // val categories = Array("electronics", "clothing", "books", "home", "sports")
    // val regions = Array("north", "south", "east", "west", "central")
    
    // å…³é”®ä¿®æ”¹ï¼šparallelize çš„ç¬¬äºŒä¸ªå‚æ•°æŒ‡å®šäº†åˆ†åŒºæ•°
    // å¦‚æœä¸æŒ‡å®šï¼Œé»˜è®¤é€šå¸¸åªæœ‰ 2 (å–å†³äºCPUæ ¸æ•°)ï¼Œä¼šå¯¼è‡´ Hash Shuffle åªèƒ½äº§ç”Ÿå¾ˆå°‘çš„æ–‡ä»¶
    val rdd = sqlContext.sparkContext.parallelize(1L to numRecords, numPartitions).map { id =>
      val rnd = new Random()
      // é¢„å…ˆç”Ÿæˆä¸€ä¸ªéšæœºçš„ byte æ•°ç»„ä½œä¸º payloadï¼Œé¿å…æ¯æ¬¡å¾ªç¯éƒ½ç”Ÿæˆå¸¦æ¥çš„ CPU å‹åŠ›
      // ä½†ä¸ºäº†é˜²æ­¢å‹ç¼©ï¼Œæˆ‘ä»¬å‡†å¤‡å‡ ä¸ªä¸åŒçš„æ¨¡ç‰ˆè½®è¯¢ä½¿ç”¨
      val payloadTemplates = (1 to 10).map { _ => 
        val bytes = new Array[Byte](1024) // 1KB
        rnd.nextBytes(bytes)
        new String(bytes, "ISO-8859-1") //ä»¥æ­¤ç¼–ç è½¬stringä¿æŒé•¿åº¦
      }.toArray

      val key = java.util.UUID.randomUUID().toString
      val value = rnd.nextDouble() * 1000
      // éšæœºé€‰ä¸€ä¸ªæ¨¡ç‰ˆ
      val bigData = payloadTemplates(rnd.nextInt(payloadTemplates.length))
      
      (key, value, "category_placeholder", bigData)
    }
    
    sqlContext.createDataFrame(rdd).toDF("key", "value", "category", "payload")
  }


  def generateSmallX(sqlContext: SQLContext): DataFrame = {
    // 10ä¸‡æ¡ * 1KB â‰ˆ 100MB æ•°æ®
    // 10 åˆ†åŒº -> Hashäº§ç”Ÿ 2000 ä¸ªæ–‡ä»¶ (æ¯ä¸ªçº¦ 50KB)
    generateUserBehavior(sqlContext, 10000L, 5)
  }

  // å®šä¹‰ä¸åŒè§„æ¨¡æ•°æ®é›†çš„é…ç½®
  def generateSmall(sqlContext: SQLContext): DataFrame = {
    // 10ä¸‡æ¡ * 1KB â‰ˆ 100MB æ•°æ®
    // 10 åˆ†åŒº -> Hashäº§ç”Ÿ 2000 ä¸ªæ–‡ä»¶ (æ¯ä¸ªçº¦ 50KB)
    generateUserBehavior(sqlContext, 100000L, 10) 
  }

  def generateMedium(sqlContext: SQLContext): DataFrame = {
    // 100ä¸‡æ¡ * 1KB â‰ˆ 1GB æ•°æ®
    // 50 åˆ†åŒº -> Hashäº§ç”Ÿ 10000 ä¸ªæ–‡ä»¶ (æ¯ä¸ªçº¦ 100KB)
    // è¿™æ—¶å€™ Hash Shuffle çš„å†™æ–‡ä»¶é€Ÿåº¦ä¼šå¼€å§‹æ˜æ˜¾å˜æ…¢
    generateUserBehavior(sqlContext, 1000000L, 50)
  }

  def generateLarge(sqlContext: SQLContext): DataFrame = {
    // 500ä¸‡æ¡ * 1KB â‰ˆ 5GB æ•°æ®
    // 100 åˆ†åŒº -> Hashäº§ç”Ÿ 20000 ä¸ªæ–‡ä»¶
    // è¿™å¯èƒ½ä¼šæŠŠä½ çš„ç£ç›˜æ‰“æ»¡æˆ–éå¸¸æ…¢ï¼Œéå¸¸é€‚åˆåšå‹åŠ›æµ‹è¯•
    generateUserBehavior(sqlContext, 5000000L, 200)
  }
  
  /**
   * ğŸ­ å·¥å‚æ–¹æ³•ï¼šæ ¹æ®ä¼ å…¥çš„ size å­—ç¬¦ä¸²åŠ¨æ€ç”Ÿæˆæ•°æ®
   * æ›¿ä»£åŸæ¥çš„ generateDatasets Map æ–¹å¼ï¼Œé¿å…ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰æ•°æ®å¯¼è‡´å†…å­˜æº¢å‡º
   */
  def generate(sqlContext: SQLContext, size: String): DataFrame = {
    size.toLowerCase match {
      case "small-x" => generateSmallX(sqlContext)
      case "small"  => generateSmall(sqlContext)
      case "medium" => generateMedium(sqlContext)
      case "large"  => generateLarge(sqlContext)
      case _ => 
        println(s"è­¦å‘Š: æœªçŸ¥çš„æ•°æ®é›†å¤§å° '$size'ï¼Œé»˜è®¤ä½¿ç”¨ small")
        generateSmall(sqlContext)
    }
  }
}